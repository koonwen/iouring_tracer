* TODO
** IO_URING_TRACER
- [ ] Add on kprobes
- [ ] Plan out dynamic loading

** ocaml-libbpf
- [X] Build libbpf and hook into c_stubs (Try using ctypes)
- [ ] Write OCaml bindings for libbpf

* Plan
** <2024-04-08 Mon>
*** AM
- [-] Figure out why C-thread cannot put events into the same ring
  buffer as user process
  ANS: It can, not a problem
  - [X] Get callbacks working in C
  - [X] Hook into bpf program without crashing
  - [X] Hook into olly
  - [X] Memlock limit, see if BPF still needs this increase explicitly
*** PM
- [X] Try to get one event written in from C
  - [X] Does C threading scheduler work pre-emptively or
    cooperatively?
    ANS: Pre-emptively. The system reserves the right
    to execute the yield command itself at any moment. In fact, it
    exercises this right sufficiently often to permit other threads to
    execute and to give the illusion that the threads are running in
    parallel, even on a uniprocessor machine.

  - [X] What happens when you don't grab the runtime lock?
    ANS: Only one thread at a time is allowed to run OCaml code on a
    particular domain, data races occur with undefined behaviour

** <2024-04-09 Tue>
*** AM
Fix IO_URING custom events not turining up in olly:
- [X] Try on simple custom event
  Ans: Yes you can see the events. Don't build with dune because it
  won't capture your program. There is a chance that events will be
  overwritten but olly will print this to stderr.

- [X] Look through olly implementation. What runtime buffer is being
  recorded? Does it track different threads

  Ans: all threads and ringbuffers are captured.  Need to watch out
  for the format type you are using fuchsia will capture all event
  Type's but json will only catch span's

- [X] Name the events
- [X] Write spans for syscalls (unpolished)

*** PM:
- [X] Write a test harness for watch mode and also to run your program
- [X] Does the bpf ring buffer drop messages?
  Ans: No

** <2024-04-10 Wed>
*** AM
- [X] Fix up development env for laptop
- [X] Produce some artifacts using custom events
  - [X] Interactive
  - [X] Against read_async.exe
  - [X] Against eio copy program
  - [X] Update README
*** PM
- [X] Add in graceful shutdown (initial work)

** <2024-04-11 Thu>
*** AM
- [X] Reply Tim about adding USDT probes
  ANS: Yes libbpf has API's to attach USDT probes.

*** PM
- [X] Investigate why there are recursive syscalls (programming error)
  Ans: with olly trace json, and interactive run, it looks like the
  callbacks are firing in the right linear order. olly trace fuchsia
  may be recording the events weirdly.

** <2024-04-12 Fri>
*** AM
- [X] Debug recursive events
  - [X] Try querying sql slices in perfetto

    ANS: Says that the time slice of the earlier call is greater and
    therefore causes the depth to increase by 1.

    Looks like the timestamps are clobbering each other. eio ts
    spans are taking on the ts recorded by the syscalls

*** PM
 - [X] Dive into runtime events implementation & olly

    ANS: timestamps are recorded in the runtime itself

  - [X] See if you can store events going into trace to debug (pin local olly)

    ANS: callback ordering is correct but issue still persists.


** <2024-04-15 Mon>
*** AM:
- Re-support separate processes for probe and user program
  - [X] Test with OCaml process that just runs bpf c program. See if
    runtime events buffer can consume events external bpf program.
  - [X] See what timestamping looks like

*** PM:
- [X] ocaml-libbpf
  - [X] Work on supporting get version stub.

** <2024-04-16 Tue>
*** AM
- [-] Continued work on libbpf
  - [ ] How does typing work in libbpf?

    ANS: size_t is the maximum addressable size of the running
    machine. It varies depending on the size of the machine word of
    the machines.

  - [X] How do you typically pass pointers into OCaml
    ANS: Not sure but can use ctypes
  - [X] Understand the header file for libbpf.h
    - [X] What does LIBBPF_API macro do
      ANS: It's just standard practice for visibility to user
    - [X] Where are the types in libbpf.h defined
      ANS: in /usr/include/linux/bpf.h

  - [-] Work toward first supporting libbpf.h & bpf_helpers.h
    - [X] libbpf_strerror
    - [ ] libbpf_bpf_attach_type_str
    - [ ] libbpf_bpf_link_type_str
      the attach_type_str functions are not that useful

    - [X] struct bpf_object
    - [X] bpf_object__open_file

*** PM
- Meeting
**** Work done recap
1. Experimented single process, multi-threaded design for getting
   probe events into same runtime buffer as OCaml user program.
2. Also experimented with multi-process, single-threaded design.
3. Hook them up with olly & testing against eio program.
4. Debugging why there are recursive syscall events recorded
   (shouldn't be the case). [Unresolved]
   Steps taken:
   - Checked if it's a programming error by injecting some printf's
     into olly to see that callbacks are fired in order. Open and close
     match and there are no double opens before we see a close.
   - Noticed that the events are only recorded in recursive fashion
     when run with fuchsia format but not with json.
   - Timestamps in perfetto show that the eio.suspend_domain event
     gets mixed up with our SYS_IO_URING_ENTER event, causing it to
     read as a recursive span. Suspect that the event registration
     ID's might overlap with each other. Changed the registration
     order but this also had no effect.
   - Spans in Fuchsia match on names? Docs are unclear, source code
     also impenetrable.
   - No tools other than perfetto for debug fuchsia format, took a
     stab at reading the binary but didn't manage to get anything out
     of that.
5. Initial work on libbpf c-bindings

**** Trade-off discussion for probe reader design
#Design1: OCaml user process + C thread w callback write to custom events
   (current implementation)

Advantages:
- Enable user to write custom bpf event callback handlers directly in
  their user program
- Supports direct usage with olly.

Disadvantages:
- Won't be able to avoid running OCaml program with root access
  because bpf programs require sudo permissions.
- Timestamps are inaccurate because they reflect when events are added
  into the ring buffer, rather than the actual time they were
  triggered. i.e. timestamps are subject to poll points when the OCaml
  user process gives up the runtime lock.

#Design2: OCaml probe process + OCaml user process + add support in olly to
   consume events from both

Advantages:
- May be possible to restrict root privileges only to the probing process
- Timestamping may be more accurate because they go into the runtime
  buffer closer to real time.

Disadvantages:
- Lining up the timestamps across processes may be non-trivial since
  they are recorded not on real-time but against the start of the
  OCaml process.
- lose the program-ability of custom bpf event handlers in the user
  program.
- requires changes to be made to olly. (In which case, we may want to
  merge the probing program directly into the olly repo)

**** libbpf stubs
- standalone library build works with dune. Any advice and convention
  for dealing with type conversions between OCaml and kernel types?
  size_t? pointers?

[208343733565115 s] rb_idx:0 (SYS_IO_URING_ENTER) BEGIN (1 open)
[208343733584743 s] rb_idx:0 (eio.suspend_domain) BEGIN (2 open)
[208343733588991 s] rb_idx:0 (SYS_IO_URING_ENTER) END   (2 close)
[208343733589141 s] rb_idx:0 (SYS_IO_URING_ENTER) BEGIN (3 open)
[208343733651008 s] rb_idx:0 (eio.suspend_domain) END   (3 close)
[208343733683259 s] rb_idx:0 (SYS_IO_URING_ENTER) END   (1 close)
   - Even when using multiple processes, recursive events are
     still appearing even though the ordering does not indicate it.

**** Other questions
- Should I ping Anil for the programs to debug?
**** TODO
Suggestions:
- emit events with fuchsia. eio_dump.
- The timestamps are based on clock_monotonic, since the system was
  booted. Should be no issue across multiple domains recording events.
- Talk to Beatrice about runtime_events tracing

bpf events use cases:
1. Check against eio programs.
2. Write bpftrace script that can give the data.

bpf events design & implementation:
1. Port eio fuchsia format to the project and emit directly.
3. Merge two trace formats.
2. Get tracing dependency out of olly.

**** Notes
When writing bpf kernel code, the SEC macro (short for section) is
used to define what libbpf program should create and how/where to
attach it in the kernel.

BPF CO-RE is a new step toward portability since BCC. Portability is
difficult with BPF because the kernel is constantly changing it's
memory layout, types can be renamed, structs can change. Using BCC
would embed your BPF program as a string and then compile them on the
fly on your production machine using CLANG and the system kernel
headers. But this is costly (size and time wise) and has a slow
iteration cycle.

BPF CO-RE relies on using BTF type information from both the BPF
program and the kernel to work out type discrepancies between
them. Libbpf works as a loader to tie the BTFs from the kernel and the
program and adjust the compiled BPF code to the specific kernel.

The user program API's for libbpf are found in public headers:
libbpf.h
bpf.h
btf.h

** <2024-04-17 Wed>
*** AM
- [-] Get usable bpftrace cmdline tool against custom script
  - [X] Update driver (support interactive use, remove runtime_events cruft)
  - [X] Fix cmdline usage of trace to be interactive
  - [X] Set up easy way to load different bt scripts
  - [X] Test against EIO http use case.

** <2024-04-18 Thu>
*** AM
- Digest Missing Manuals IO_uring & investigate slowness
  - [X] Where does asynchronous work come in io_uring runtime

    ANS: The spawning of io_workers are dependent on the type of io
    work called. If the io request has a non-blocking option, it
    preferentially takes the code path that calls this and sets a
    wakeup notification to read when it's done. No new worker thread
    is spawned. However, if the IO request only has a blocking api,
    then this code path spawns new worker threads to concurrently
    process the blocking requests.

    However, if you want to force uring to use the blocking path to
    spawn worker threads, this is possible by marking requests into
    uring as IOSQE_ASYNC which tells it to issue requests
    asynchronously.

    The other aspect to consider is the number of workers that uring
    will spawn in response. Uring describes how it automatically
    determines this. But we can also manually set the number of max
    workers using IO_URING_REGISTER_IOWQ_MAXWORKERS.

    There are other ways to set this limit which requires setting
    RLIMIT_NPROC or configuring cgroup process limit.

  - [X] show if worker threads are created

*** PM
  - [X] try to spawn more worker threads.
    ANS: liburing doesn't seem to allow us to configure spawning
    worker threads.

  - [X] figure out why there are lost events

    ANS: bpftrace user program is single threaded and processes events
    one by one, the kernel side buffer therefore drops events because
    it the user side cannot clear the buffer fast enough. We can
    increase the size of the kernel buffer by increasing
    BPFTRACE_PERF_RB_PAGES config variable.

** <2024-04-19 Fri>
*** AM
  - [X] Convert loading of bt files via paths so that config variables
    work.
    - ANS: It doesn't matter if it's by script or inline, both don't work
  - [X] Submit Weeklies

*** PM
  - [X] Add in config params to bpftrace



** <2024-04-22 Mon>
*** AM
- [X] Port eio fuchsia format to the project and emit directly.
  - [X] Swap out serialization engine to use Faraday
  - [X] Hook up with lwt loop

*** PM
- [X] Port contd.
- [X] Write bpftrace script showing depth
**** Notes
liburing usage optimization:
https://github.com/axboe/liburing/wiki/io_uring-and-networking-in-2023

** <2024-04-23 Tue>
*** AM
- [X] bpftrace with depth
  - [X] Get timestamping correct
- [X] Check if the print format provided in the event spec can be used
  directly
- [X] Fix timestamp for libbpf version

*** PM
- [X] Merging Fuchsia trace formats
  - ANS: Just a simple cat. But perfetto sometimes messes up the
    spans, so it's a good idea to make sure the thread id or category
    is different
- [X] See if you can use libtrace directly
  - ANS: looks hard to extract.

** <2024-04-24 Wed>
*** AM
- [X] Add more bpftrace programs
  - [X] uring_spans: depth log
  - [X] uring_ops: op count & time
  - [X] uring_strace_summary: summary of syscalls on multiple ring instances
  - [X] uring_tp_strace: strace-like output

*** PM
- [ ] Parsing for bpftrace output for nicer printing

** <2024-04-25 Thu>
Hacking days
*** AM
- [X] Pick a task
  ANS: Chose ocaml-crunch project
- [X] Read about how its done in paper
- [X] Review how its done in rust & golang
- [X] See how it's done in ocaml-crunch

*** PM
**** Meeting
***** Work done
- Port eio_trace fuchsia format writer into our project.
- Write strace & strace summary equivalent bpf scripts for bpftrace
- Investigate io-uring performance intricacies

***** For discussion
- What to prioritize?
  1. Write more bpftrace scripts for tracing uring
     - Good: Simple, fast, effective
     - Bad: Not much of an OCaml library, really just a collection of
       scripts. Cannot output fxt, not extensible
  2. Work on libbpf bindings for backend supporting tracing & bpf
     usage
     - Good: extensible, can integrate directly into OCaml.
     - Bad: A lot more work to get the same results as bpftrace

- Performance engineering questions for io-uring
  - Are eio fibers seen as systhreads? Not recommended to share a
    io-uring instance between multiple threads.
    https://github.com/axboe/liburing/issues/571#issuecomment-1106480309)
  - How much faster performance are we expecting? Online benchmarks
    are divided on whether io-uring is strictly faster than
    epoll. Looks like we have baseline expectations for read & writes
    for the uring bindings but they aren't compared against posix
    backends. We should probably look at doing those first before
    trying speed up eio?
  - syscall latency wise, even though we have substantially less
    syscalls, the time spent inside them is still overall about same
    as posix backends. On my machine however, as the number of domains
    go up, the throughput increases in favour of io-uring. At low core
    counts, performance can dip worse than posix


***** Integrate eio_trace fuchsia format writer into uring trace library
- Hacky, swapped out serialization engine from eio to faraday
  because needed to register a callback and I'm not sure if it's
  possible with eio's capabilities design pattern. Also uses lwt as
  backend to manage concurrency, may want to do a rewrite of this to
  use effects

***** Write OCaml frontend for bpftrace usage
- Make it easy to drop-in different bpftrace scripts to be loaded &
  run

***** Investigating io-uring performance on eio http server
- Question: How much faster performance are we expecting? Online
  benchmarks are divided on whether io-uring is strictly faster than
  epoll.

- `strace -c` shows that syscall count for io-uring is substantially
  lower but the time spent per call is slower. total time spent in
  kernel ends up being almost identical.

- I'm interested in bench-marking purely the uring bindings and see if
  there optimization is there. It seems like there a bunch of knobs to
  twiddle when it comes to using uring effectively but those choices
  are abstracted by the uring bindings.

** <2024-04-26 Fri>
*** AM
- [X] Try to get original tracer to register callback
- [ ] Debug Fuchsia trace not being written.
  ANS: Uncompleted, consolidated and asked for suggestions

*** PM
- [X] Work on libbpf bindings
  - [X] Work on the enums for the libbpf_set_strict_mode ANS: flagsets
    aren't well supported by ctypes, only allowed to pass one flag
    instead of a list
  - [X] Work on bpf_object__open
    ANS: Works for loading an object file
    but not sure if the memory is allocated correctly
  - [X] Work on bpf_object__load
    ANS: Implementation works, Load performs implicit increase in RLIMIT_MEMLOCK


** <2024-04-29 Mon>
*** AM
- [X] Complete libbpf bindings surface area
  - [X] libbpf_bpf_get_error
  - [X] bpf_object__find_program_by_name
  - [X] bpf_object__next_program
  - [X] bpf_program__attach
  - [X] bpf_object__find_map_by_name
  - [X] bpf_map__fd
  - [X] bpf_link__destroy
  - [X] bpf_object__close
  - [X] ring_buffer__new
  - [X] ring_buffer__poll
  - [X] ring_buffer__free
- [X] Get working OCaml user program

  ANS: Needed to indicate to Foreign.funptr that optional
  ~runtime_lock:true so that in the callback it reacquires the runtime
  lock

*** PM
- [X] Port iouring_tracer to use libbpf bindings
  - [X] Get structs mapping working in Ctypes
  - [X] Get Fxt writer working

** <2024-04-30 Tue>
*** AM
- [X] Fix timestamping

*** PM
- [ ] Fix types & tid
- [ ] Investigating user/kernel time difference (kernel time is the same, but posix backend is slower in user space)
  - [ ] Disable the mitigation's & run (Expectation is that posix backend should catch up)
  - [ ] Try perf to see the user space slowness. (need frame pointers)
  - [ ] Use magic trace to see where the time is being spent (only on intel machine)

* NEXT Meeting
<2024-05-02 Thu>
** TODO
- [ ] Work on libbpf bindings.
- [ ] Investigating user/kernel time difference (kernel time is the same, but posix backend is slower in user space)
  - [ ] Use magic trace to see where the time is being spent
  - [ ] Try perf to see the user space slowness.
  - [ ] Disable the mitigation's & run (Expectation is that posix backend should catch up)
- [ ] Extract fxt tracing into standalone library
- [ ] Work on Anil's benchmarks in uring repo.

* Backlog:
** Tracing analysis
- [ ] Implement spawning more worker threads
- [ ] write a test suite to check the number of processes, threads and
  io-workers for the given target
- [ ] show tasks sent for async request vs tasks sent for polling
- [ ] show completion events
- [ ] Convert eio benchmark program into a polling server.
- [ ] See what kind of performance bonus io_uring can maximally perform

** iouring_tracer
- [ ] Add graceful shutdown C-thread design
- [ ] Setup Co-RE, Replace <bpf/linux.h> with "vmlinux.h"
- [ ] Figure out how to separate running of the root bpf program and
  OCaml. Might need to be an external monitor like bpftrace
- [ ] Add custom events polling to bpftrace
- [ ] Add epoll method
- [ ] How does the Event module work?
- [ ] Check what low impact monitoring using C primatives for runtime
  events can do
- [ ] Potentially add USDT probes
- [ ] Runtime lock blocks until a poll point. This messes up the
  custom events timestamp, see if it's possible to extend custom
  events to support this
- [ ] Work towards packaging and giving a talk about the tool to the internal teams
- [ ] Work on getting the timestamping correct in custom events.
  - [ ] What is the semantics of caml_runtime_acquire_lock
  - [ ] See if it's possible to pass on timestamps
  - [ ] Check how often the runtime lock is being grabbed
  - [ ] See if you can buffer events

** Libbpf
- [ ] Wrap one function from libbpf using vendoring method
  - [ ] Check initial commit for liburing
  - [ ] How does compilation process work for simple C library.
    - [ ] Static
    - [ ] Dynamic
    - [ ] Includes
    - [ ] Headers

* Notes with Sadiq:
- Previously, the strace like io-uring tracer I demo-ed, utilizes
  bpftrace to bootstrap writing and loading a bpf program that logs
  io-uring events. io-uring events here could be anything from
  io-uring syscalls, static kernel tracepoints, kprobes, etc. Bpftrace
  makes it easy to hook into these points in the kernel and write
  small programs that execute when these points are hit.
  Disadvantages:
  1. Getting these logs into OCaml is a bit hacky because we get
     bpftrace process to write to separate file and get out OCaml
     process to periodically read them.
  2. This means we rely on bpftrace third party external dependency,
     which is not as extensible as we would like.

Decided to migrate from using bpftrace to libbpf implement our bpf
program (How does bpftrace work?)
  Advantages:
  - More portable
  - More fine-grain control over how the kernel program works and
    better API's like bpf-ringbuffer.
  - Instead of using file indirection to get write and read events. We
    can use the ringbuffer which maps between user and kernel
    space. Then I use OCaml callbacks to write into custom events
  Disadvantages:
  - Not easy to write, and carries along the static build of libbpf -
    incurs long build time

- Currently, I have a minimal working example for the libbpf backend
  - that hooks into io_uring tracepoints and writes these events via the
   callbacks.

  Questions:
- My custom events are appearing. Do these traces look reasonable to you?

- Need more use-cases to see whether the tool is providing useful
  trace work, It's a bit of guesswork to see what probes to add.

- To write the c-bindings library for libbpf, is it preferable to
  vendor the c-library as a stand-alone or require it as a system
  dependency? I see that the uring library does the former but not
  sure what's good practice.

- I tried using ctypes stub generation to implement the bindings but
  got some undefined symbols error, It looks like the support for
  ctypes stub generation is not particularly active. Is it worthwhile
  trying to debug this or go for the hand written stubs?

- About the C API's for zero impact monitoring. If I use them, will
  olly have trouble visualizing this info?
- About the implementation, I used callbacks instead of the polling
  method to get the notifications into OCaml

- Investigate why there are recursive syscalls (programming error)
- Work towards packaging and giving a talk about the tool to the internal teams
- runtime lock blocks until a poll point
- What kind of data are we looking for in custom events? Just the
  instance point or something more?
